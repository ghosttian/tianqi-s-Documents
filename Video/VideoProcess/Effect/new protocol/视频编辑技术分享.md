# wbvideo-editor相关分享

## wbvideo架构

首先我们先看整个wbvideo的整体结构图

![wbvideo结构图](http://oon83udyw.bkt.clouddn.com/%E6%96%B0%E8%A7%86%E9%A2%91%E5%B1%82%E7%BA%A7.png)

## 时间轴（Timeline） 
我们在Editor中虚拟出了一条时间轴，然后由系统控制，逐帧进行处 理、渲染。

### 结构

Timeline 内部由多个Stage组成，每个[Stage](#stage)含有多个[Action](#action)，Stage主要的职责是解码，Action的主要职责是图像处理，如图所示。

![结构图](http://oon83udyw.bkt.clouddn.com/Timeline%E7%BB%93%E6%9E%84.png)


由于时间轴是有时间的，并且是顺序进行逐帧渲染，那么我们将结构图进行矢量化，来看一下在二维坐标之下的Timeline的结构和流程。

![矢量结构图](http://oon83udyw.bkt.clouddn.com/Timeline%E7%BB%93%E6%9E%84%EF%BC%88%E7%9F%A2%E9%87%8F%EF%BC%89.png)
### 处理一帧的流程
![时序图](http://oon83udyw.bkt.clouddn.com/%E4%B8%80%E6%AC%A1%E6%B8%B2%E6%9F%93%E6%97%B6%E5%BA%8F%E5%9B%BE.png)
### 生命周期
![生命周期](http://oon83udyw.bkt.clouddn.com/Timeline%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F.png)

<span id="stage"/>
## 舞台（Stage）

Stage类在整个SDK中担任的职责类似一个视频解析器。


```
{
    "stageId": "1",
    "videoPath": "/storage/emulated/0/Android/videos/spring.mp4",
    "videoResource": "1",
    "videoStartPoint": 30000,
    "stageLength": 10000,
    "timelinePoint": 0,
    "actions": [
        {
            "actionName": "NoEffectAction",
            "actionId": "1",
            "stagePoint": 0,
            "actionLength": 6000
        }
    ]
}
```

* 每个Stage中都有一个唯一Id，用于Timeline找到它。对应属性：`stageId`
* 每个Stage中有一个视频源，Stage把该视频源拆分成单一的视频帧。对应属性：`videoPath`、`videoResource`
* 每个Stage都有视频起点，时间长度，以及在Timeline上对应的时间点。对应属性：`videoStartPoint `、`stageLength`、`timelinePoint`
* 每个Stage都有一个或多个Action，用来对视频帧进行渲染。对应属性：`actions`


### 生命周期

Stage的生命周期有：

1. prepare：当Timeline进行prepare时进行调用
2. onAdded：当Stage进入前台时调用（GL线程）
3. onRender：当Stage进行渲染时调用（GL线程）
4. onProcess：当Stage进行处理时调用（GL线程）
5. onRemoved：当Stage退出前台时调用（GL线程）

### 视频抽帧

在视频抽帧这里，在渲染与处理上，是两种不同的抽帧方式。渲染是异步提前抽帧，保证了渲染线程尽可能少的阻塞，而处理则是同步抽帧，保证了视频帧和音频帧的顺序性。

<span id="action"/>
## 特效（Action）

Action类在SDK中的职责是图像处理，不同的Action具有不同的处理功能。

基类是抽象类`BaseAction`

```
{
    "actionName": "GrayAction",
    "actionId": "1",
    "stagePoint": 0,
    "actionLength": 6000,
    "inputType": "raw",
    "inputId": "1"
}
```

* 每个Action都有自己的名称，SDK通过该名称去生成Java实例。扩展的Action需要将自己的名称注册到`JsonToJavaProtocol`中。对应属性：`actionName`
* 每个Action中都有一个唯一Id，用于Stage内部存储并找到它。对应属性：`actionId`
* 每个Action都有时间长度，以及在Stage上对应的时间点。对应属性：`stagePoint`、`actionLength`
* 每个Action可以选择它的输入源（默认为系统赋予的输入源）。对应属性：`inputType`、`inputId`

### 生命周期
Action的生命周期有：

1. onAdded：当Action进入前台时调用（GL线程）
2. onRender：当Action进行渲染时调用（GL线程）
3. onRemoved：当Action退出前台时调用（GL线程）

### OpenGLAction
`OpenGLAction`继承于`BaseAction`，其内部基于OpenGL，实现了最基本的图形渲染。
一些简单的滤镜效果，可以通过继承自`OpenGLAction`，然后替换其片段着色器，修改一些值，来实现滤镜。**需要注意的是其内部已经定义类着色器的参数的名称，所以，在着色器时，请将着色器内的参数与其名称进行对应，或者通过重写`initShaderHandle`方法自行设置着色器的句柄。**

#### 相关方法
* `onAdded`：该方法是`BaseAction`的抽象方法的实现。其代码逻辑主要是初始化OpenGL的Program，以及初始化片段着色器的句柄。
* `onRender`：该方法是`BaseAction`的抽象方法的实现。其代码逻辑主要是OpenGL的绘制相关。
* `onRemoved`：该方法是`BaseAction`的抽象方法的实现。其代码逻辑主要是删除Program。
* `setShader`：该方法负责设置顶点着色器和片段着色器，需要在子类的构造函数中进行调用。
* `getProgram`：调用该方法会返回`OpenGL`的`program`句柄。
* `onPreDrawArrays`：该方法是在`onRender`方法中，执行`glDrawArrays`之前被调用。子类可重写该方法，对`OpenGL`进行一些状态设置。
* `setInteger`：该方法会在渲染线程中调用`OpenGL`的`glUniform1i`。
* `setFloat`：该方法会在渲染线程中调用`OpenGL`的`glUniform1f`。
* `setFloatVec2`：该方法会在渲染线程中调用`OpenGL`的`glUniform2fv`。
* `setFloatVec3`：该方法会在渲染线程中调用`OpenGL`的`glUniform3fv`。
* `setFloatVec4`：该方法会在渲染线程中调用`OpenGL`的`glUniform4fv`。
* `setFloatArray`：该方法会在渲染线程中调用`OpenGL`的`glUniform1fv`。
* `setPoint`：该方法会在渲染线程中调用`OpenGL`的`glUniform2fv`。
* `setUniformMatrix3f`：该方法会在渲染线程中调用`OpenGL`的`glUniformMatrix3fv`。
* `setUniformMatrix4f`：该方法会在渲染线程中调用`OpenGL`的`glUniformMatrix4fv`。
* `runOnDraw`：该方法会在渲染线程中执行`Runnable`的`run`方法。


### MultipleInputsAction
`MultipleInputsAction`继承自`OpenGLAction`，其内部重写了`OpenGLAction`的`onRender`方法，将渲染一个纹理改为渲染多个纹理，并且支持平移、缩放、旋转、透明度这几项参数的设置以及对应的动画设置。

关于`MultipleInputsAction`，我们可以按照移动端开发的角度去理解这个`Action`。如果普通的`Action`相当于不同的`View`，那么`MultipleInputsAction`就相当于是一个`ViewGroup`，它主要负责的是将不同的输入纹理进行旋转，平移，缩放，透明度等参数的设置，然后进行布局，最后显示到屏幕或是FrameBuffer上。

之所以不特定一些个`Action`来进行旋转，平移，缩放等，是因为超出了纹理范围的话，可能会影响实际展示效果。  

#### JSON语法

```
{
    "actionName": "MultipleInputsAction",
    "actionId": "1",
    "stagePoint": 0,
    "actionLength": 10000,
    "inputs": [
        {
            "type": "raw",
            "id": "1",
            "spiritX": -0.5,
            "spiritY": -0.5,
            "spiritScale": 0.5,
            "spiritRotationAngle": 0,
            "spiritAlpha": 1
        },
        {
            "type": "raw",
            "id": "1",
            "scale": [
                {
                    "keys": [
                        0,
                        5000
                    ],
                    "values": [
                        1,
                        0.5
                    ]
                }
            ],
            "translation": [
                {
                    "valueSize": 2,
                    "keys": [
                        0,
                        5000
                    ],
                    "values": [
                        0,
                        0,
                        0.5,
                        0.5
                    ]
                }
            ]
        }
    ]
}
```

在`MultipleInputsAction`中新增的参数就是`inputs`

`inputs`是个数组，里面每一个JSONObject都是一个输入源。对应的里面的一些参数就是这些输入源的一些属性以及动画。

通过`type`和`id`来确定输入源的纹理ID，剩下的一些X、Y、rotation、translation之类的参数都是用来定义输入源的一些参数的。

## 渲染上下文（RenderContext）
RenderContext在SDK中的职责是保存渲染所需要的相关信息，组成渲染上下文。RenderContext是只读的。

### 获取时间相关
* `getTimestamp`：获取当前帧时间
* `getLastTimestamp`：获取上一帧时间
* `getNextTimestamp`：获取下一帧时间

### 获取资源相关
获取资源这里是需要通过`type`和`id`来定位具体的资源ID，`type`的类型有以下几种：

1. `TEXTURE_TYPE_DEFAULT`：默认类型，通常为上一个`Action`处理完成后的纹理ID，ID默认为空字符串。
2. `TEXTURE_TYPE_RAW`：视频原生资源，其ID对应的就是`Stage`的id。
3. `TEXTURE_TYPE_STAGE`：`Stage`处理完成后的视频资源，其ID对应的就是`Stage`的id。
4. `TEXTURE_TYPE_ACTION`：`Action`处理完成后的视频资源，其ID对应的就是`StageId,ActionId`。
5. `TEXTURE_TYPE_IMAGE`：图片资源，此处还未开发，TODO。

* `getDefaultTexture`：获取默认纹理ID
* `getTexture`：获取制定资源ID，入参是`type`和`id`。
* `isTypeAvailable`：RenderContext的静态方法，检查`type`是否是符合要求的。

### 获取位置相关
* `getStageIndex`：获取当前`Stage`在前台`Stage`中所处的位置
* `getStageSize`：获取前台`Stage`的数量
* `getActionIndex`：获取当前`Action`在前台`Action`中所处的位置
* `getActionSize`：获取前台`Action`的数量

<span id="keyframes"/>
## 关键帧动画（Keyframes）

Keyframes在SDK中主要是用来实现动画效果的。

```
[
    {
        "valueSize": 1,
        "keys": [
            0,
            5000
        ],
        "values": [
            1,
            0.5
        ],
        "interpolator": {
            "name": "LinearInterpolator"
        }
    }
]
```

* `valueSize`：int值，该值表示一个Key对应几个Value，默认为1。
* `keys`：long数组，表示时间节点
* `values`：double数组，表示时间节点对应的值
* `interpolator`：插值器

值得注意的是，这里是一组关键帧数组。之所以是数组，是考虑到可能相同的关键帧属性，可能会有不同的关键帧参数，比如说前3秒钟使用匀速位移，在3-6秒的时候使用减速位移。使用数组就可以解决这种情况的动画效果。

### 动画实现
利用关键帧来实现动画，核心原理是通过每一帧时间的流逝，计算出对应的数值，然后通过不同数值的渲染，来实现动画效果。

核心方法`public double[] getValue(long key)`  
输入一个时间`key`，通过内部的插值器进行计算，返回一组结果的`double[]`数组，其长度是由`valueSize`决定的。

### 插值器
插值器目前只提供线性插值器，即`LinearInterpolator`。

如果需要自己写一些特定的插值器，例如：加减速插值器，贝塞尔插值器等，需要继承自`AbstractInterpolator`，并实现其内部的抽象方法`getValue`，然后将其注册在`JsonToJavaProtocol`中，即可在JSON中进行使用。

## 关于Stage和Action分离的想法

未来的关于JSON的构思（基于QIM的思考）

删除掉关于视频合成相关的一些参数，只关注资源和特效

![](http://oon83udyw.bkt.clouddn.com/JSON%E4%BF%AE%E6%94%B9%E6%96%B9%E5%90%91.png)

```
{
    "jsonVersion": "v0.1.0.0",
    "resource": {
        "video": [
            {
                "id": "1",
                "name": "xxx",
                "path": "/sdcard/xxx/xxx.mp4"
            }
        ],
        "image": [
            {
                "id": "1",
                "name": "xxx",
                "path": "/sdcard/xxx/xx/xx.png"
            }
        ],
        "text": [
            {
                "id": "1",
                "content": "text",
                "size": 10,
                "argb": [0,0,0,0],
                "font": "xxx",
                "bold": false,
                "italic": false
            }
        ],
        "audio": [
            {
                "id": "1",
                "path": "/sdcard/xxx/xxx.mp3",
                "audioStartPoint": 0,
                "stageLength": 10000,
                "timelinePoint": 0
            }
        ]
    },
    "timeline": {
        "stages": [
            {
                "id": "1",
                "resource": "1",
                "videoStartPoint": 0,
                "length": 10000,
                "timelinePoint": 0
            }
        ],
        "music": [
            {
                "id": "1",
                "resource": "1",
                "audioStartPoint": 0,
                "length": 10000,
                "timelinePoint": 0
            }
        ]
    },
    "actions": {
        "videoActions": [
            {
                "actionName": "OpenGLAction",
                "actionId": "1",
                "stagePoint": 0,
                "actionLength": 10000
            }
        ],
        "audioActions": [
            {
                "actionName": "AudioMixAction",
                "actionId": "1",
                "stagePoint": 0,
                "actionLength": 10000
            }
        ]
    }
}
```
